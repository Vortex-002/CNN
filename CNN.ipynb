{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vortex-002/CNN/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hi! Welcome to my project.\n",
        "> Here you will get to see that how write simple code in colab can make it possible to rule out skin diseases efficiently though this project is purely for educational purposes and should not be used to actually diagnose any disease.\n",
        "\n",
        ">Every code block will have a text block preceding it to explain what happens in it.\n",
        "> Link to the Dataset: https://www.kaggle.com/datasets/adityush/eczema2"
      ],
      "metadata": {
        "id": "CBW9kKfIw3qT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Mean and Standard Deviation\n",
        "\n",
        "Here I am calculating the mean and standard deviation to rule out the normalizing facotrs which are mean and standard deviation for the normalising transform applied on the dataset."
      ],
      "metadata": {
        "id": "eqZrX5e3xrRy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6Dj6PJULopy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "dataset = datasets.ImageFolder(root='/content/drive/MyDrive/data', transform=transform)\n",
        "\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "mean = 0.0\n",
        "std = 0.0\n",
        "total_images = 0\n",
        "\n",
        "\n",
        "for images, _ in tqdm(loader, desc=\"Calculating mean and std\", unit=\"batch\"):\n",
        "    batch_samples = images.size(0)\n",
        "    images = images.view(batch_samples, images.size(1), -1)\n",
        "    mean += images.mean(2).sum(0)\n",
        "    std += images.std(2).sum(0)\n",
        "    total_images += batch_samples\n",
        "\n",
        "\n",
        "mean /= total_images\n",
        "std /= total_images\n",
        "\n",
        "print(f'Mean: {mean}')\n",
        "print(f'Standard Deviation: {std}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader Block\n",
        "- I applied transforms to the images as in data augmentation to alter the image so that the model learns better\n",
        "- I load the dataset\n",
        "- I split the dataset with `random_spilt` in 80 10 10 format\n",
        "- Then separated them into batches"
      ],
      "metadata": {
        "id": "GBfspQjT-cyJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs7HycaPK_Ru"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.6314, 0.5293, 0.4935], [0.1907, 0.1813, 0.1858])\n",
        "])\n",
        "\n",
        "\n",
        "dataset = datasets.ImageFolder(root='/content/drive/MyDrive/data', transform=transform)\n",
        "\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THE MODEL\n",
        "\n",
        "> This is the model architecture a detailed explanation of how it works is in: https://docs.google.com/document/d/1hQK2o809k89_jgKlYjPn3Y3VJqX87nfTIfLZN6BNaLo/edit?tab=t.0\n",
        "\n",
        "> To sum it up briefly\n",
        "-  3 convolutional layers (9-17)\n",
        "-  LSTM (22)\n",
        "-  Fully connected layers (25 - 27)\n",
        "-  Forward function with: (29-46)\n",
        "      - Feature extraction (31-33)\n",
        "      - RNN reshaping\n",
        "      - The actual RNN\n",
        "      - FC + Dropout + Output"
      ],
      "metadata": {
        "id": "gwzy7bLs_KC3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qaLHLbkwTL1k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_Model, self).__init__()\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 2, 64)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "\n",
        "        b, c, h, w = x.size()\n",
        "        x = x.permute(0, 2, 3, 1).contiguous()\n",
        "        x = x.view(b, -1, c)\n",
        "\n",
        "\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Train Block\n",
        "> Here I declare the optimizer as SGD with momentum and also we take Binary Cross Entropy Loss for our loss function. I have kept number of epochs as 10.\n"
      ],
      "metadata": {
        "id": "JuKnxTpfBU_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kLbiAxZYSGg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "weight_decay = 0.01\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN_Model().to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", ncols=100)\n",
        "\n",
        "    for inputs, labels in loop:\n",
        "        inputs, labels = inputs.to(device), labels.float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        loop.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"\\n Epoch {epoch+1}: Avg Loss = {avg_loss:.4f}, Accuracy = {acc:.2f}%\")\n",
        "\n",
        "    torch.save(model, \"cnn_model.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparison code\n",
        "This section is purely for comparison with an optimiser as Adam"
      ],
      "metadata": {
        "id": "Ym5jl5i2CedE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3qNiXlR1QTp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.01\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN_Model().to(device)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", ncols=100)\n",
        "\n",
        "    for inputs, labels in loop:\n",
        "        inputs, labels = inputs.to(device), labels.float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        loop.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"\\nEpoch {epoch+1}: Avg Loss = {avg_loss:.4f}, Accuracy = {acc:.2f}%\")\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_acc:.2f}%\\n\")\n",
        "\n",
        "\n",
        "torch.save(model, \"cnn_adam_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Metrics Block\n",
        "> Here we find out the:\n",
        "  - Accuracy\n",
        "  - Precision\n",
        "  - Recall\n",
        "  - F1 Score\n",
        "  - Specificity\n",
        "  - ROC AUC\n",
        "  - ROC Plot\n",
        "  - Confusion Matrix"
      ],
      "metadata": {
        "id": "kM5WbyQdCyP9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LRIChGe4Is9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_path = \"/content/drive/MyDrive/cnn_model.pt\"\n",
        "model = torch.load(model_path, map_location=device, weights_only=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, threshold=0.5):\n",
        "    model.eval()\n",
        "    y_true, y_pred, y_probs = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
        "            outputs = model(inputs).squeeze()\n",
        "            probs = outputs.cpu().numpy()\n",
        "            preds = (outputs > threshold).float().cpu().numpy()\n",
        "            y_probs.extend(probs)\n",
        "            y_pred.extend(preds)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_probs = np.array(y_probs)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    auc = roc_auc_score(y_true, y_probs)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    specificity = tn / (tn + fp + 1e-6)\n",
        "\n",
        "    print(f\"Accuracy    : {acc:.4f}\")\n",
        "    print(f\"Precision   : {prec:.4f}\")\n",
        "    print(f\"Recall      : {rec:.4f}\")\n",
        "    print(f\"F1 Score    : {f1:.4f}\")\n",
        "    print(f\"Specificity : {specificity:.4f}\")\n",
        "    print(f\"ROC AUC     : {auc:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Eczema'], yticklabels=['Normal', 'Eczema'])\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ROC\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "evaluate_model(model, val_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is the test metrics code block\n",
        " - All I have done here is that I have evaluated the model based on the evaluation metrics used above but only for the test set."
      ],
      "metadata": {
        "id": "RdumWKsGHGHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_path = \"/content/drive/MyDrive/cnn_model.pt\"\n",
        "model = torch.load(model_path, map_location=device, weights_only=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, threshold=0.5):\n",
        "    model.eval()\n",
        "    y_true, y_pred, y_probs = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
        "            outputs = model(inputs).squeeze()\n",
        "            probs = outputs.cpu().numpy()\n",
        "            preds = (outputs > threshold).float().cpu().numpy()\n",
        "            y_probs.extend(probs)\n",
        "            y_pred.extend(preds)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_probs = np.array(y_probs)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    auc = roc_auc_score(y_true, y_probs)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    specificity = tn / (tn + fp + 1e-6)\n",
        "\n",
        "    print(f\"Accuracy    : {acc:.4f}\")\n",
        "    print(f\"Precision   : {prec:.4f}\")\n",
        "    print(f\"Recall      : {rec:.4f}\")\n",
        "    print(f\"F1 Score    : {f1:.4f}\")\n",
        "    print(f\"Specificity : {specificity:.4f}\")\n",
        "    print(f\"ROC AUC     : {auc:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Eczema'], yticklabels=['Normal', 'Eczema'])\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ROC\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "id": "rEkGHcpZocb0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1DBhyjA2bPl-7ahzJYged32uEpgxLQk4k",
      "authorship_tag": "ABX9TyPhx82DnsYFcP0E3UOLVgat",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}